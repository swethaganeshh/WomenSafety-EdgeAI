# ğŸ›¡ï¸ Women Safety App â€“ AI-Powered Scream Detection  
### Edge Impulse Integrated Â· Real-Time Distress Monitoring Â· Automatic SOS

Hey there! This project is an impactful blend of **AI**, **mobile development**, and **safety innovation** â€” built to protect women in real-world emergency situations. With **Edge Impulseâ€™s on-device ML**, the app listens for distress cues like *screams*, *â€œhelpâ€*, and sudden loud noises, and triggers **automatic SOS actions** â€” even when the user cannot tap a button.

---

## ğŸŒŸ What This Project Solves

Women often face unsafe situations where they:

âŒ Cannot press an SOS button  
âŒ Cannot unlock their phone  
âŒ Cannot call for help  
âŒ Panic or freeze  
âŒ Are physically restricted  

Traditional safety apps depend on manual triggers â€” which fail when they matter most.

---

## ğŸš¨ Problem Statement

There is a need for a safety system that:

- âš¡ Works automatically  
- ğŸ§ Detects danger in real time  
- ğŸ†˜ Sends alerts without user input  
- ğŸ“¡ Works even offline  
- ğŸ›Ÿ Helps women get immediate support  

This project solves that using **AI-based scream detection**.

---

## ğŸ’¡ Proposed Solution

A mobile app powered by **Edge Impulseâ€™s audio classification model** that detects:

- ğŸ”Š Scream-like audio  
- ğŸ—£ï¸ Distress keywords (â€œHelpâ€, â€œStopâ€)  
- ğŸ”‰ Sudden loud noises  
- ğŸ™ï¸ Unusual audio patterns  

When danger is detected â†’ **Automatic SOS**, **GPS sharing**, **audio recording**, and **contact alerts**.

---

## ğŸ”¥ Key Features

- ğŸ¤– AI-powered scream detection (â‰¥75% confidence)  
- ğŸ§  Real-time MFCC + CNN audio classification  
- ğŸš¨ Automatic SOS activation  
- ğŸ“ Live GPS sharing  
- ğŸ“¡ Works offline (on-device inference)  
- ğŸ“± Background monitoring  
- ğŸ¤ Auto audio evidence recording  
- ğŸ“³ Fall/struggle detection via accelerometer  

---

## ğŸ› ï¸ Tech Stack

### ğŸ“± Mobile App
- Flutter / React Native  
- Google Maps API  
- Firebase / Twilio / WhatsApp API  

### ğŸ¤– AI Model
- Edge Impulse Audio Classification  
- MFCC Feature Extraction  
- TensorFlow Lite Deployment  

### ğŸ”Œ Sensors & Integrations
- Microphone  
- GPS  
- Accelerometer  

---

## ğŸ”— Edge Impulse Integration

### 1ï¸âƒ£ Data Collection
Includes:
- Scream samples  
- Crying / shouting  
- Background noise  
- Talking  
- Silence  
- Keywords: â€œHelpâ€, â€œStopâ€  

### 2ï¸âƒ£ Model Training
- MFCC extraction  
- CNN classifier  
- Labels: Scream, Noise, Talking, Silence  
- Accuracy: **82â€“90%**  

### 3ï¸âƒ£ Deployment
- Exported as `.tflite`  
- Edge Impulse C++ SDK  

### 4ï¸âƒ£ Mobile Model Output Example

```json
{
  "Scream": 0.82,
  "Talking": 0.10,
  "Noise": 0.05,
  "Silence": 0.03
}
```

### Output
<img width="959" height="401" alt="image" src="https://github.com/user-attachments/assets/a6ba1881-ff19-49dc-970f-a3b1abae9f52" />

<img width="955" height="401" alt="image" src="https://github.com/user-attachments/assets/9446c826-dfd3-476b-b94b-342388be13cd" />

<img width="956" height="398" alt="image" src="https://github.com/user-attachments/assets/6e67fe5f-247e-40f0-8080-8980bb01a602" />




